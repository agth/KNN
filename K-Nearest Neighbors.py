"""
================================
K-Nearest Neighbors Classification
Медот K-ближайших соседей
================================
"""

print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 15 #K-целое значение, указанное пользователем. 
iris = datasets.load_iris() # выборка с ответами 

X = iris.data[:, :2] #присваиваем X значения из выборки iris
# выборка iris описана в библиотеке sklearn
# выборка iris представляет таблицу 150 строк и 4 стоблца
# первое двоеточие означает что берём все 150 строк
# :2 означает что берём только 2 стоблца из всех что есть, в данном случае из 4-х
	# X = iris.data[:, :]  все строки и (признаки) столбцы
	# X = iris.data[:2, :]  первые 2 строки и все (признаки) столбцы
y = iris.target #массив ответов. ответ на каждую точку может быть один из трёх

h = 0.02  # размер шага в сетке
# Создание цветовых карт
cmap_light = ListedColormap(['#bafffc', '#ffdede','#abffbd']) #цвета площадок
cmap_bold = ListedColormap(['Blue', 'Red', 'Lime']) #цвета точек


def knn(weights_):
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights = weights_)#объявляю экземпляр класса
    clf.fit(X, y) # помещает в экземпляр класса данные
    #нужно найти размер сетки, необходимый для посторения
    #для этого пробегаемся по всем значениям выборки X
    #находим максимальные и минимальные значения и прибавляем отнимаем 1 чтобы взять с небольшим запасом
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1#находим минимальное и максимальное 
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    #X[:, 0].min() минимальное значение из выборки X[:, 0] 
    # выборка  X[:, 0] - один стоббец из выборки с четырьмя столбцами X[:, :]
    # X[:, 1] по вертикали
    # X[:, 0] по горизонтали
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),  np.arange(y_min, y_max, h))
    #  ф-я возвращает 2 значения и присваеивает двум переменным xx yy
    #print (xx)
    #print (20 * "-")
    #print (yy)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])    
    # Z хранит ответы для каждой точки на плоскости 3 цвета 3 разных значения
    p = np.c_[xx.ravel(), yy.ravel()]
    # np.c_ объединяет в один массив два массива
    # Поместим результат в цветовую схему
    #print (Z)
    Z = Z.reshape(xx.shape)
    
    #print (xx.shape)
    #print (Z)
    plt.figure() #рисует 'uniform' . без этой строки рисует только 'distance'
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)#отрисовка цветов фона плоскости

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='black', s=20)#отрисовка точек на плоскости
        #edgecolor='black' цвет ободков вокруг точек.
    plt.title("3-Class classification (k = %i, weights = '%s')"% (n_neighbors, weights_ ))#отрисовка заголовка
    plt.show() #показать на экране рисунки

#при прогнозировании к какому классу относится участок можно использовать различные параметры весовой функции
#'uniform' означает равномерную фесовую функцию. Рассояние от классифицируемого объекта до заданных точек не учитываеется
#учитывается только объектов (точек) какого класса больше
#'distance' означает что учитывается не только количество объектов (точек) того или иного класса до
#классифицируемого объекта но и расстояние на котором они расположены до классифицируемого объекта
#вызываю функцию, подставляя различные параметры весовой функции

knn('uniform')#вклад в классификацию каждой точки одинаковый 
knn('distance')#Используется весовая функция расстояния
